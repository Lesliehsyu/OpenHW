I"X<p>本作品旨在设计能够在硬件端实现 AI 编曲的 SoC，对复调音乐编曲并进行实 时推理和播放。其中以 Arm Cortex-m0 为控制器内核，对多种板载外设进行调度， 同时搭载基于 GRU 的神经网络硬件加速器，用来推理和谐的音符序列，最后由音频 DAC 实现音频输出。</p>

<h1 id="一种含基于gru网络硬件加速器的ai编曲soc">一种含基于GRU网络硬件加速器的AI编曲SoC</h1>

<p>作者：相博镪；胡双；邹金成</p>

<p> </p>

<p><strong>一、设计概述</strong></p>

<p> </p>

<p><strong>1.1 设计目的</strong></p>

<p>本作品旨在设计能够在硬件端实现 AI 编曲的 SoC，对复调音乐编曲并进行实 时推理和播放。其中以 Arm Cortex-m0 为控制器内核，对多种板载外设进行调度， 同时搭载基于 GRU 的神经网络硬件加速器，用来推理和谐的音符序列，最后由音频 DAC 实现音频输出。</p>

<p><strong>1.2 应用领域</strong></p>

<p>基于 FPGA 平台将神经网络模型部署到硬件，能够适用于某些室外场景下的即兴伴奏，或是聆听音乐地需求。并允许对编曲没有基础的人，在任意时刻能够最快获得一段只属于自己的和谐音乐。</p>

<p><strong>1.3 主要技术特点</strong></p>

<p>设计中核心部分是基于 GRU 模型的硬件运算加速器模块。针对神经网络模型大量乘加运算的特点，我们使用 16 路 16bit 乘法器作为并行乘法阵列，在 80MHz 的时钟频率下实现高速运算。 系统以 arm cortex-m0designstart eval 作为软核 cpu，并搭载多个外设。其中 TF 卡预先存储了 46 个音符对应的 5 秒音源，以及 PC 端训练得到的约 26 万个 神经网络模型参数；DDR3 SDRAM 在系统初始化时将 TF 卡内容读入作为缓冲。 针对 GRU 较为复杂的运算流程和数据调度，我们为硬件加速器设计了一组 轻量的指令集，cpu 凭藉于此，通过写寄存器的方式，命令加速器实现数据加载、运算等一系列动作，降低纯 HDL 设计的复杂度，同时提高了模块的可复用性。</p>

<p><strong>1.4 主要创新点</strong></p>

<p>其中神经网络使用一种基于 GRU 和 Embedding 模型的 TonicNet 网络，这种网络较好地兼顾了编曲的乐理特点和更低的运算复杂度，在生成音乐较为和谐的同时，也非常适合使用硬件实现。</p>

<p>（1）指令集架构控制数据流向和运算流程。为了更好地兼顾设计中的细节， 更好地将数据流的转移和运算高度契合在一起，我们放弃了 HLS 等高层次设计工具，而是以 Veriog 编写整个模块，其中仅利用了基础加法器、乘法器，FIFO 以及 DDR3 控制器 IP。同时嵌入了指令集的思想，使得整个运算模块能够通过 CPU 的 总线指令进行调度，在保留高速运算特性的同时又不失灵活易重构易调度的特点。</p>

<p>（2）将 AI 编曲迁移到硬件实现。传统 AI 编曲往往运行在或部署了 GPU 的计算机，而本作品将推理部署到 FPGA 芯片后，联合片外存储器，声卡芯片即能和其他电声设备共同工作。</p>

<p> </p>

<p><strong>二、系统组成及功能说明</strong></p>

<p> </p>

<p><strong>2.1 整体介绍</strong></p>

<p>根据设计需求，我们设计了如图 1 所示的 SoC。 ARM Cortex-M0 软核处理器作为主控，通过 AHB 总线对各个外设模块实现控制。</p>

<p><img src="/OpenHW/assets/images/article_9/image_1.png" alt="image_1" /></p>

<p>TF 卡预先保存了 PC 端训练的神经网络模型参数和 46 个音符对应的 5 秒音源，在系统启动时，CPU 通过 SPI 读取数据并搬移到 SDRAM；2 组 GPIO 读按 键、拨码开关、控制 LED 灯；i2c 与 i2s 用来和声卡芯片交互；DDR3 SDRAM 控制器选择仲裁其他设备的数据请求。</p>

<p>神经网络的运算流程以 TonicNet 为原型，并根据硬件特点进行实现。其中数据以 16-bit 有符号定点数保存和运算，并能够利用指令配置小数点位置，用以在数值区间和精度间自由取舍；Sigmoid 和 tanh 激活函数部分使用流水线的结构进行，以二次多项式拟合的方式实现；后处理时的指数运算由于对精度要求不高，因此直接使用查找表实现；多项分布部分使用 LFSR 产生随机数，再由累加的形式来模拟按概率取值；神经网络计算完毕后交由 cpu 进行后处理，将结果和音符形成映射关系，再取出音符对应音源，最后由音频 DAC 输出。</p>

<p><strong>2.2 UART/SPI/GPIO/TIMER</strong></p>

<p>我们的 SoC 搭载了两个 UART 外设。其中 UART0 用来向用户打印 log 信息， UART1 用来和外界传输数据。PC 端利用 pytorch 库读取神经网络模型参数文件，并利用 pyserial 库来实现串口发送，与此同时硬件系统通过 UART1 实现和 PC 的数据交互，并通过 SPI 将数据写入 TF 卡相应扇区。</p>

<p><img src="/OpenHW/assets/images/article_9/image_2.png" alt="image_2" /></p>

<p>同样，我们利用 matlab 读取 46 个音源的 wav 文件，并将其波形转换为 16-bit 有符号数，通过 UART1 写入 TF 卡相应扇区。利用 SD 卡作为大容量移动存储器，可以便捷地调整实际应用中需要的模型参数和音乐播放音源，也正契合了我们在硬件部署 AI 编曲后随用随听的目的。</p>

<p><strong>2.3 DDR3 SDRAM 控制器</strong></p>

<p>我们的 DDR3 SDRAM 控制器基于 vivado 的 MIG IP，为了实现多设备控制仲 裁，以及位宽的转换，我们在 MIG 的基础上进行封装。</p>

<p><img src="/OpenHW/assets/images/article_9/image_3.png" alt="image_3" /></p>

<p>在我们的系统中，存在 3 个设备需要直接访问 SDRAM。cpu 在系统初始化时 通过 AHB 总线将 SD 卡中的数据缓存至 SDRAM；神经网络模块在计算过程中需要不断地从 SDRAM 中读取模型参数；音频播放模块需要由要播放的音符类型， 来从 SDRAM 中取出音源数据，缓冲后交由音频 DAC 播放。</p>

<p><strong>2.4 音频 DAC 控制器</strong></p>

<p>在初始化时，需要通过 I2C 来对音频 D/A 芯片进行配置，我们可以通过软件写寄存器的方式，配置好工作模式、音量、采样率等参数后，再利用 I2C 完成写入。</p>

<p><img src="/OpenHW/assets/images/article_9/image_4.png" alt="image_4" /></p>

<p>在需要进行音乐播放时，软件程序写控制模块寄存器，表明需要播放的音 源在 SDRAM 中的储存地址，以及要播放的样本数，控制模块将数据读入 FIFO 及逆行缓冲，最终通过 I2S 向音频芯片发送数据，在开发板 line out 接口插入耳机或音响，即可聆听最终的音乐输出。</p>

<p>作品使用的模型生成复调音乐，因此需要在同一时刻对多个音符取音源， 合成后进行播放。软件程序通过写参数寄存器和指令寄存器的方式，告知音频 控制模块需要播放的音符在 SDRAM 中存储的位置，播放长度，以及对应的轨道，接下来播放模块判断轨道对应 FIFO 的状态，以在合适的时机取出音源数据 播放。</p>

<p><strong>2.5 神经网络模型和加速器模块</strong></p>

<p>随着近些年深度学习的飞速发展，AI 编曲领域也日益高涨。较为闻名的有 已经将之商业化的 AVIV，以及 Google 开源的 Magenta。其中大多以基于 RNN， CNN 或其变体为主，并按编曲风格融入某些乐理规则进行正则。要生成和谐的 音乐，通常对模型要求较为复杂，google 的 wavenet 模型利用因果卷积直接生成音频波形，magenta 也大多使用注意力机制，但在硬件实现这些模型时，要比传统的 CNN 和 RNN 在计算流程上要复杂得多。我们评估了大量主流的 AI 编 曲模型后，选择了一种模型相对简单，效果较为出色的模型作为我们硬件实现 的原型。</p>

<p>2.5.1 模型仿真</p>

<p>我们使用 TonicNet 模型作为原型，这是一种基于 RNN 的神经网络模型，首先使用词嵌入（embedding 模型）为输入音符数据建模，接下来利用三层 output channel 为 256 的 GRU 网络进行推理，再通过一层全连接层得到神经网 络的输出，最后融入乐理规则对输出结果后处理，得到四条音轨，作为最终的 编曲输出。如图 5 所示，为某次模型输出 midi 标准的数据后，利用 musescore3 软件解析出的乐谱。</p>

<p><img src="/OpenHW/assets/images/article_9/image_5.png" alt="image_5" /></p>

<p>GRU 和 LSTM 是 RNN 网络的变体，为解决 RNN 长期记忆和反向传播中的梯 度等问题而来。相较于 LSTM，GRU 通常能够以更精简的计算结构，得到相似的实验效果。</p>

<p><img src="/OpenHW/assets/images/article_9/image_6.png" alt="image_6" /></p>

<p>当前时刻的输入 Xt 和上一时刻的输出 Ht-1 被输入进 GRU 模型，并送入更新门 rt 和重置门 zt。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，重置门控制前一状态有多少信息被写入到当前的候选集<img src="/OpenHW/assets/images/article_9/image_15.png" alt="image_15" />上。</p>

<p><img src="/OpenHW/assets/images/article_9/image_7.png" alt="image_7" /></p>

<p>公式(2-1)至公式(2-4)构成了一个标准的 GRU 模型结构。在实际应用中，不 同的功能需求下对模型有着不同的要求。如图 7，在本作品使用的模型中，主 要可以分为三部分：</p>

<ol>
  <li>
    <p>词嵌入模型。模型的输入有两个部分，xt和 zt。xt作为当前时刻的输入， 是由前一时刻的输出迭代而来，zt是由上一时刻的输出特性，经过后处理而来。得到 xt和 zt后，将其映射为特定向量的形式，这一过程称为词嵌入（word embedding）。</p>
  </li>
  <li>
    <p>GRU 模型。将词嵌入后映射得到的向量输入至 GRU 模型。本作品中使用 了三层 GRU，输入通道为 288，输出通道为 256，每层 GRU 的运算结果分别作为当前时刻下一层的输入 xt，以及下一时刻当前层的输入 ht，以此类推在整个 网络中进行传递。</p>
  </li>
</ol>

<p><img src="/OpenHW/assets/images/article_9/image_8.png" alt="image_8" /></p>

<ol>
  <li>输出层和后处理。第三层 GRU 网络将某一时刻运算结果输出至全连接层 如公式(2-5)，得到维度为 98 的向量输出。至此神经网络部分在当前时刻运算完毕。而后处理，则是将输出向量的 98 个元素先分别进行指数运算实现非线性映射，接下来按概率随机抽取，即最终得到当前时刻的输出音符，最后再按照旋律的行进规则进行调整，作为神经网络下一时刻的输入。</li>
</ol>

<p>2.5.2 硬件实现</p>

<p>选定模型后，我们以此为原型进行硬件设计。大体有如下几点：</p>

<ol>
  <li>
    <p>对原模型进行 16-bit 量化。利用 pytroch 读出在 PC 端预先训练好的参数 文件，并仿真统计出运算过程中最值后，将参数转换为 16-bit 定点数，我们选择一位符号位，三位整数位，和十二位小数位。其中在计算输出层时，需要调节为五位整数位，十位小数位来防止溢出，这一步可使用配置寄存器的方式自 动实现。完毕后，按照 2.2 节中阐述的方式，将定点化后的参数写入 Micro SD 卡。</p>
  </li>
  <li>
    <p>设计运算单元。类似于线性层的运算结构实际上就是矩阵乘法，各通道 间的运算是独立且规整的，因此非常适合利用并行运算做加速。我们综合考量了模型结构、需要的运算速度，以及 FPGA 可利用资源，最终选择使用 16 路 16-bit 并行乘法阵列作为加速器的核心，在 80MHz 的时钟频率下通过 3 个 cycle 得到乘积，再以加法树的形式得到乘加结果。除此之外累加运算模块、sigmoid 运算模块、tanh 运算模块以流水线的方式进行。为了进一步提高速度，Sigmoid 和 tanh 我们使用二次多项式分段拟合的方式，而不是传统的 CORDIC；考虑到 计算过程中对指数运算精度要求不高，本工程中直接取查找表实现；最后需要 按概率对输出结果向量取索引，这里我们先对输出向量求和，同时利用 LFSR 产 生伪随机数，将随机数对和取模后，判断模值在向量的位置，即可类似于取样得到的所求索引。</p>
  </li>
  <li>
    <p>数据流控制。如图 8 所示，cpu 通过对寄存器写入指令的方式来控制整个运算流程。首先，cpu 向加速器模块各参数寄存器写值，来配置指令将要操 作的地址、数据量和操作模式，写入完毕后对指令寄存器写指令，经加速器模块控制器解析后，来启动所要进行的运算流程。</p>
  </li>
</ol>

<p><img src="/OpenHW/assets/images/article_9/image_9.png" alt="image_9" /></p>

<p>首先，cpu 告知加速器，本轮计算的输入值所对应的词嵌入向量在 SDRAM 中的地址，加速器将其取出到缓冲 data_in 中。再依次取出当前计算所需要的 bias 和 weight 到相应缓冲，由控制器根据指令按相应的方式进行并行乘加运 算。所有的运算结果汇总到缓冲 data_out 中，其中缓冲 data_tmp0 和 data_tmp1 能在某些特殊运算中自动拷贝 data_out 的运算结果。缓冲 data_r， data_z 是 GRU 运算流程中需要的特殊缓冲，data_h 在 GRU 每层计算完毕后更新其运算结果。当当前时刻的 gru 和输出层均计算完毕后，进入硬件实现的最终阶段，将结果向量元素分别进行指数运算，并将结果作为其索引所对应的概率，按照多项分布的形式模拟从中抽取，将抽取结果的索引作为当前时刻硬件端的最终结果，通过中断的形式告知 cpu 本时刻运算完毕。</p>

<p><strong>2.6 软件程序</strong></p>

<p>在本系统中，cpu 负责启动时对所有模块初始化、控制调度，以及计算结果的后处理。</p>

<p><img src="/OpenHW/assets/images/article_9/image_10.png" alt="image_10" /></p>

<p>当整个系统复位，首先程序声明后处理需要用到的变量，接下来对 UART、 TIMER、GPIO、SPI、音乐播放模块、DDR3 SDRAM 控制器模块，以及神经网络加速器模块进行初始化，完毕后串口打印欢迎信息。</p>

<p>各模块初始化完毕后，进入工作前的准备阶段。CPU 利用程序从 SD 卡读出 神经网络模型参数数据，以及各音符对应的音源，写入 DDR3 SDRAM，以便后续工作状态下能够将数据高速读出。</p>

<p>数据准备完毕后，程序赋予神经网络一个初始值，功能开始运作起来。在本程序中，神经网络加速器模块并不是始终运行，而是为播放功能做数据准备。程序循环查询播放模块的状态寄存器，当播放缓冲的数据低于一个阈值，程序就启动神经网络加速器模块运算，得到结果后将要播放的数据放入播放模块缓冲中， 以此来实现整个系统实时播放的目的。</p>

<p> </p>

<p><strong>三、完成情况及性能参数</strong></p>

<p> </p>

<p>本作品的设计功能全部完成，包括 SoC 结构搭建、软件程序设计、存储器控 制部分、音乐播放部分，以及神经网络加速器模块。最终通过 cpu 软件调度，SoC系统将神经网络加速器运算得到的 MIDI 标准音符，通过播放模块实时输出到音频 DAC 芯片，利用耳机或音响连接开发板的 LINE OUT 音频接口即能够聆听到 AI 编曲作乐。</p>

<ol>
  <li>
    <p>SoC 架构完成。使用 Arm cortex-m3designedstart 作为软核 cpu，工作在 40MHz 时钟频率下。外设部分 UART、SPI、GPIO、timer 使用了 Verilog IP；DDR3 SDRAM 控制模块在 vivado MIG IP 的基础上进行封装，使得多个设备能够访问存储器；音频 DAC 控制模块在 i2c 和 i2s 的基础上加入 FIFO，封装 AHB 接口以及其 他逻辑实现配置和播放；神经网络加速器模块和软件程序完全由我们设计，并最终将整个系统连接起来。</p>
  </li>
  <li>
    <p>神经网络运算部分完成。对所选模型进行评估，我们的模型原型 TonicNet 网络在其论文中提到了仍存在的问题。其模型以 JSB 作为训练数据集，这是一种 四声部和声的数据集，对应了生成标准 midi 格式的四个轨道。在此基础上提出 了多种训练模型，如图 10 展示了以 16 分音符为时间序列步长下，多种基于 TonicNet 的模型在 JSB 训练集下的损失。其中 C 表示和弦（Chord），S 表示高音声部（Soprano），A 表示中音声部（Alto），T 表示次中音（Tenor），B 表示低音声 部（Bass）；NCL 即 No Chord Loss，表示仅对 note 音符进行了评估；Tr 和 MM 分别表示对数据集进行了和弦转位，以及进行了大调-小调转换来增强数据集。</p>
  </li>
</ol>

<p><img src="/OpenHW/assets/images/article_9/image_11.png" alt="image_11" /></p>

<p>论文中还提到了输出结果仍然会存在某些 16 分音符存在偏离其对应声部的趋势，以致产生不和谐的装饰音效果。使用集束搜索有时生成序列过短。有时会突然发生大调转小调或者相反的情况。这些问题可能是由于使用 teacher forcing 训练 TonicNet 时由于误差的存在而导致的棋盘效应。</p>

<p>我们对 TonicNet 的运算结构进行评估，并将其作为原型进行硬件实现。</p>

<p><img src="/OpenHW/assets/images/article_9/image_12.png" alt="image_12" /></p>

<p>在实际硬件实现过程中，16 路 16-bit 并行乘法阵列在 80MHz 时钟频率下以 流水线的方式进行运算。对于输入通道为 256 的一组向量，每个乘积求和得到一 次并行乘法运算结果，需要消耗不少于 16 个时钟周期，再与 Bias 和前次结果累加，实现完整的 W*x+B 运算。</p>

<p>我们作品中选择 XC7A75TFGG484 作为 FPGA 芯片，最终实现后，资源利用如 图 11 所示：</p>

<p><img src="/OpenHW/assets/images/article_9/image_13.png" alt="image_13" /></p>

<ol>
  <li>音频播放部分基本完成。音频 DAC 芯片使用 WM8978，将以 16 位有符号 数保存的音源数据输出到开发板 line out 接口。在播放过程中，尽管以 16000Hz 采样率对音源播放足够，但输出的模拟信号存在直流偏置，实际播放效果存在稍明显的噪音，这也是我们后续工作中要继续调试的部分。</li>
</ol>

<p><img src="/OpenHW/assets/images/article_9/image_14.png" alt="image_14" /></p>

<p> </p>

<p><strong>四、总结</strong></p>

<p> </p>

<p><strong>4.1 可拓展之处</strong></p>

<p>由于开发时间仓促，我们虽然完成了最初设想的功能，但在这个过程中，我们不断地发现可以继续优化的地方，其中既包括对加速器速度和计算架构的优化， 也有如神经网络模型，以及仿真和开发流程的优化。这也是我们接下来的工作中 索要继续进行的。</p>

<ol>
  <li>
    <p><strong>并行乘法阵列改为流水线方式计算。</strong>我们当前使用的计算结构中，数据读入、数据运算以及结果数据的三个过程在执行中需要等待前一过程执行完毕，因此运算还不能足够密集。在接下来的工作中，我们将以流水线的操作方式优化这 一部分设计。</p>
  </li>
  <li>
    <p><strong>加速器对指令的执行过程加入缓冲方式。</strong>在本系统中，软件程序向加速器 写入一条指令完毕后，需要通过查询状态寄存器或等待中断的方式来得知当前指令执行完毕，才能写入下一条指令。但实际上可以使用指令缓冲的方式，即 cpu 一次写入多条指令，加速器悉数接收后放入缓冲，等待当前指令执行完毕后自动 从缓冲中读入下一条指令，这样既解放了 cpu，也提高加速器自身响应指令的速 度。</p>
  </li>
  <li>
    <p><strong>优化存储器控制模块。</strong>在本系统中有多个设备访问了 DDR3 SDRAM 存储 器。其中加速器模块和播放模块可以在工作状态下由仲裁器和选择器直接高速访问 SDRAM，但 CPU 在准备阶段需要利用 spi 从 Micro SD 卡向 SDRAM 搬运数据， 这一步受限于程序的运行速度，因而拖慢了准备过程。后续我们将优化 SPI 到 SDRAM 间的控制，提高数据流速度。</p>
  </li>
  <li>
    <p><strong>调整加速器架构，提高兼容性。</strong>最初为了解决 GRU 相较于传统线性网络、 卷积网络以及循环网络更复杂的结构，我们设计了指令集的形式结构，来降低纯 verilog 开发复杂度的同时，提高了灵活性和配置性。但实际上现在所用的指令操 作中仍有不合理的地方，在接下来的工作中，我们将调整指令集结构，以在运算 更高效的前提下，提高对其他网络的兼容性，能够适用于更多的网络模型结构。</p>
  </li>
  <li>
    <p><strong>继续添加音源，实现对电子音乐设备的支持。</strong>由于我们采用的是神经网络生成 midi 标准音符，再利用播放器播放音源的方式实现，这样就能兼容大量电子音乐设备，比如可直接连接电脑或混音器，得到更丰富的音乐合成效果。</p>
  </li>
  <li>
    <p><strong>使用更丰富的训练集，优化乐理规则约束，训练更多风格的音乐。</strong>本系统 中我们以 Tonicnet 模型为原型实现硬件加速，但实际上 AI 编曲相关模型远远不局限于此。后续我们将继续尝试进行其他相关模型的硬件加速实现。</p>
  </li>
</ol>

<p>将神经网络加速器集成到 SoC 具有较好的前景和实用性。在设计过程中，可 以根据不同需求来定制如 GPIO、UART、音频播放、VGA 显示等外设模块。而由 于不同神经网络模型也会有大量相似类型的运算，如乘加是基础，一般还会有池化、激活函数等运算，这些相通的运算类型非常适合集成到神经网络加速器中做 运算加速，除此之外像预处理和后处理有时逻辑复杂但运算量不大，这部分任务 可以交给 cpu 完成，最终软硬结合来实现整个运算流程。</p>

<p> </p>
:ET